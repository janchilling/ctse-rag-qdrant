{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System with Qdrant and Gemini\n",
    "\n",
    "This notebook implements a complete Retrieval Augmented Generation (RAG) system using:\n",
    "- **Qdrant** (in Docker) as the vector database\n",
    "- **SentenceTransformers** for embeddings\n",
    "- **Gemini** as the LLM\n",
    "- **LangChain** to connect everything\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "1. Install Docker Desktop for Windows\n",
    "2. Start Qdrant in Docker with:\n",
    "   ```\n",
    "   docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v \"%cd%\\qdrant_storage:/qdrant/storage\" qdrant/qdrant\n",
    "   ```\n",
    "3. Create a folder named `pdfs` containing your PDF documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\SLIIT-Y4-Sem2-modules\\CTSE-SE4010\\Assigments\\ctse-rag-qdrant\\ctse-qdrant-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Environment\n",
    "\n",
    "Set up the Google API key for Gemini. You can store it in a `.env` file or input it directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google API key configured\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set Google API key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    # Uncomment and input your key if not using .env\n",
    "    # GOOGLE_API_KEY = \"your_google_api_key_here\"\n",
    "    pass\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(f\"‚úÖ Google API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Embedding Model\n",
    "\n",
    "We'll use the SentenceTransformers library with the `all-MiniLM-L6-v2` model, which produces 384-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keshan Pathirana\\AppData\\Local\\Temp\\ipykernel_15500\\4273172181.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoder ready, dim = 384\n"
     ]
    }
   ],
   "source": [
    "# Set up the embedding model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embedding_dimension = embeddings.client.get_sentence_embedding_dimension()\n",
    "print(f\"‚úÖ Encoder ready, dim = {embedding_dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connect to Qdrant and Create Collection\n",
    "\n",
    "Connect to the Qdrant server running in Docker and set up the vector collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created new Qdrant collection: pdf_documents\n",
      "üåê Qdrant dashboard available at: http://localhost:6333/dashboard\n"
     ]
    }
   ],
   "source": [
    "# Connect to Qdrant Docker container\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"pdf_documents\"\n",
    "\n",
    "# Check if collection exists, if not create it\n",
    "collections = qdrant_client.get_collections().collections\n",
    "collection_names = [collection.name for collection in collections]\n",
    "\n",
    "if collection_name in collection_names:\n",
    "    # Delete collection if it exists\n",
    "    qdrant_client.delete_collection(collection_name=collection_name)\n",
    "    print(f\"üóëÔ∏è Deleted existing collection: {collection_name}\")\n",
    "\n",
    "# Create new collection\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"‚úÖ Created new Qdrant collection: {collection_name}\")\n",
    "print(f\"üåê Qdrant dashboard available at: http://localhost:6333/dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Split PDF Documents\n",
    "\n",
    "We'll load PDFs from the `./pdfs` directory and split them into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2023-S1-SE4020-Lecture-02-Introduction.pdf...\n",
      "Loading 2025-S1-SE4020-Lecture-01-Introduction.pdf...\n",
      "‚úÖ Loaded and split 31 chunks from 2 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# Set up folder path and text splitter\n",
    "folder_path = \"./pdfs\"  # folder containing your PDFs\n",
    "loader_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Created directory: {folder_path}\")\n",
    "    print(\"Please add your PDF files to this directory and run this cell again.\")\n",
    "else:\n",
    "    # Load and process PDF files\n",
    "    documents = []\n",
    "    pdf_files = 0\n",
    "    \n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_files += 1\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        print(f\"Loading {fname}...\")\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load_and_split(text_splitter=loader_splitter)\n",
    "        documents.extend(pages)\n",
    "\n",
    "    if pdf_files == 0:\n",
    "        print(f\"No PDF files found in {folder_path}. Please add PDF files and run this cell again.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Loaded and split {len(documents)} chunks from {pdf_files} PDF files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Vector Store and Add Documents\n",
    "\n",
    "We'll create embeddings for all document chunks and store them in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and uploading to Qdrant...\n",
      "‚úÖ Uploaded 31 document chunks to Qdrant in 2.97 seconds\n",
      "üåê View your collection at: http://localhost:6333/dashboard/#/collections/pdf_documents\n"
     ]
    }
   ],
   "source": [
    "# Check if documents were loaded before proceeding\n",
    "if 'documents' in locals() and len(documents) > 0:\n",
    "    # Create vector store\n",
    "    start_time = time.time()\n",
    "    print(\"Creating embeddings and uploading to Qdrant...\")\n",
    "    \n",
    "    vectorstore = Qdrant.from_documents(\n",
    "        documents,\n",
    "        embeddings,\n",
    "        url=\"http://localhost:6333\",\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Uploaded {len(documents)} document chunks to Qdrant in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"üåê View your collection at: http://localhost:6333/dashboard/#/collections/{collection_name}\")\n",
    "else:\n",
    "    print(\"No documents loaded. Please run the previous cell successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Up the LLM and Prompt Template\n",
    "\n",
    "We'll use Google's Gemini Pro model and create a prompt template for consistent answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template created\n"
     ]
    }
   ],
   "source": [
    "# Set up the LLM (Gemini)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            temperature=0.2,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"You are an expert assistant. Use the following context (with page numbers) to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "1. Summary:  \n",
    "   Provide a succinct explanatory summary (1‚Äì2 sentences).\n",
    "\n",
    "2. Key Points:  \n",
    "   List the main supporting details in bullet form. For each bullet, cite the page number in parentheses.\n",
    "\n",
    "Example format:\n",
    "\n",
    "1. Summary:  \n",
    "   The primary purpose of vector databases is to store and query dense vector embeddings for similarity search (page 12).\n",
    "\n",
    "2. Key Points:  \n",
    "   - Vector databases offer fully managed services, eliminating infrastructure overhead (page 5).  \n",
    "   - They support cosine and dot-product similarity metrics for fast nearest-neighbor retrieval (page 8).  \n",
    "   - They integrate seamlessly with popular embedding libraries like SentenceTransformer (page 14).  \n",
    "   - They provide automatic indexing to scale to millions of vectors (page 20).\n",
    "\n",
    "Now, answer the question below following this format:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create the RAG Chain\n",
    "\n",
    "Now we'll connect all components to create our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain with Gemini is ready.\n"
     ]
    }
   ],
   "source": [
    "# Check if vectorstore exists\n",
    "if 'vectorstore' in locals():\n",
    "    # Create RetrievalQA chain\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG chain with Gemini is ready.\")\n",
    "else:\n",
    "    print(\"Vectorstore not created. Please run previous cells successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test with Example Questions\n",
    "\n",
    "Let's test our RAG system with some example questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\SLIIT-Y4-Sem2-modules\\CTSE-SE4010\\Assigments\\ctse-rag-qdrant\\ctse-qdrant-venv\\lib\\site-packages\\langchain_google_genai\\chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QUESTION: What are ranges?\n",
      "==================================================\n",
      "\n",
      "ANSWER:\n",
      "1. Summary:\n",
      "Ranges in Swift define a sequence of values, and Swift provides several operators to create different types of ranges, including closed, half-open, and one-sided ranges. These ranges can be countable (integers) or strideable, allowing for enumeration or stepping through values with a specific increment (page 16).\n",
      "\n",
      "2. Key Points:\n",
      "   - Closed Range Operator (a...b): Includes both 'a' and 'b' (page 16).\n",
      "   - Half-Open Range Operator (a..<b): Includes 'a' but not 'b' (page 16).\n",
      "   - One-Sided Ranges (a... or ...b): Represents a range from 'a' to the end or from the beginning to 'b' (page 16).\n",
      "   - Countable Range: Ranges of integers that can be enumerated (a..<b or a...b where a and b are integers) (page 16).\n",
      "   - Strideable Range: Values stepped through with a certain stride (stride(from: a, to: b, by: s) or stride(from: a, through: b, by: s)) (page 16).\n",
      "\n",
      "SOURCES:\n",
      " ‚Ä¢ ./pdfs\\2023-S1-SE4020-Lecture-02-Introduction.pdf ‚Äî page 15\n",
      " ‚Ä¢ ./pdfs\\2023-S1-SE4020-Lecture-02-Introduction.pdf ‚Äî page 16\n",
      " ‚Ä¢ ./pdfs\\2023-S1-SE4020-Lecture-02-Introduction.pdf ‚Äî page 1\n"
     ]
    }
   ],
   "source": [
    "# Define a function to handle questions\n",
    "def ask_question(question):\n",
    "    if 'qa_chain' not in locals() and 'qa_chain' not in globals():\n",
    "        print(\"RAG chain not created. Please run previous cells successfully first.\")\n",
    "        return None\n",
    "    \n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nANSWER:\")\n",
    "    print(result[\"result\"])\n",
    "    \n",
    "    print(\"\\nSOURCES:\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        src = doc.metadata.get(\"source\", \"unknown\")\n",
    "        pg = doc.metadata.get(\"page\", \"unknown\")\n",
    "        print(f\" ‚Ä¢ {src} ‚Äî page {pg}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try an example question\n",
    "example_question = \"What are ranges?\"\n",
    "result = ask_question(example_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Mode\n",
    "\n",
    "Use this cell to ask custom questions about your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your own question\n",
    "your_question = input(\"What would you like to ask about your documents? \")\n",
    "result = ask_question(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exploring the Vector Space in Qdrant Dashboard\n",
    "\n",
    "You can explore your vectors in the Qdrant dashboard:\n",
    "\n",
    "1. Open http://localhost:6333/dashboard in your browser\n",
    "2. Go to \"Collections\" and click on the `pdf_documents` collection\n",
    "3. Use the \"Search\" tab to perform vector searches\n",
    "4. View the \"Cluster view\" to visualize your vector space\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Some ideas to improve your RAG system:\n",
    "\n",
    "1. Adjust the chunk size and overlap for better context retrieval\n",
    "2. Try different embedding models for improved relevance\n",
    "3. Experiment with different prompt templates\n",
    "4. Implement metadata filtering to search specific documents or sections\n",
    "5. Add logging to track performance and improve the system over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctse-qdrant-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
