{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System with Qdrant and Gemini\n",
    "\n",
    "This notebook implements a complete Retrieval Augmented Generation (RAG) system using:\n",
    "- **Qdrant** (in Docker) as the vector database\n",
    "- **SentenceTransformers** for embeddings\n",
    "- **Gemini** as the LLM\n",
    "- **LangChain** to connect everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Environment\n",
    "\n",
    "Set up the Google API key for Gemini. You can store it in a `.env` file or input it directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set Google API key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    # Uncomment and input your key if not using .env\n",
    "    # GOOGLE_API_KEY = \"your_google_api_key_here\"\n",
    "    pass\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "print(f\"‚úÖ Google API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Embedding Model\n",
    "\n",
    "We'll use the SentenceTransformers library with the `all-MiniLM-L6-v2` model, which produces 384-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embedding_dimension = embeddings.client.get_sentence_embedding_dimension()\n",
    "print(f\"‚úÖ Encoder ready, dim = {embedding_dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connect to Qdrant and Create Collection\n",
    "\n",
    "Connect to the Qdrant server running in Docker and set up the vector collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Qdrant Docker container\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"pdf_documents\"\n",
    "\n",
    "# Check if collection exists, if not create it\n",
    "collections = qdrant_client.get_collections().collections\n",
    "collection_names = [collection.name for collection in collections]\n",
    "\n",
    "if collection_name in collection_names:\n",
    "    # Delete collection if it exists\n",
    "    qdrant_client.delete_collection(collection_name=collection_name)\n",
    "    print(f\"üóëÔ∏è Deleted existing collection: {collection_name}\")\n",
    "\n",
    "# Create new collection\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE),\n",
    ")\n",
    "print(f\"‚úÖ Created new Qdrant collection: {collection_name}\")\n",
    "print(f\"üåê Qdrant dashboard available at: http://localhost:6333/dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Split PDF Documents\n",
    "\n",
    "We'll load PDFs from the `./pdfs` directory and split them into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder path and text splitter\n",
    "folder_path = \"./pdfs\"  # folder containing your PDFs\n",
    "loader_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Created directory: {folder_path}\")\n",
    "    print(\"Please add your PDF files to this directory and run this cell again.\")\n",
    "else:\n",
    "    # Load and process PDF files\n",
    "    documents = []\n",
    "    pdf_files = 0\n",
    "    \n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "        pdf_files += 1\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        print(f\"Loading {fname}...\")\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load_and_split(text_splitter=loader_splitter)\n",
    "        documents.extend(pages)\n",
    "\n",
    "    if pdf_files == 0:\n",
    "        print(f\"No PDF files found in {folder_path}. Please add PDF files and run this cell again.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Loaded and split {len(documents)} chunks from {pdf_files} PDF files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Vector Store and Add Documents\n",
    "\n",
    "We'll create embeddings for all document chunks and store them in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if documents were loaded before proceeding\n",
    "if 'documents' in locals() and len(documents) > 0:\n",
    "    # Create vector store\n",
    "    start_time = time.time()\n",
    "    print(\"Creating embeddings and uploading to Qdrant...\")\n",
    "    \n",
    "    vectorstore = Qdrant.from_documents(\n",
    "        documents,\n",
    "        embeddings,\n",
    "        url=\"http://localhost:6333\",\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Uploaded {len(documents)} document chunks to Qdrant in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"üåê View your collection at: http://localhost:6333/dashboard/#/collections/{collection_name}\")\n",
    "else:\n",
    "    print(\"No documents loaded. Please run the previous cell successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Up the LLM and Prompt Template\n",
    "\n",
    "We'll use Google's Gemini Pro model and create a prompt template for consistent answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM (Gemini)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            google_api_key=GOOGLE_API_KEY,\n",
    "            temperature=0.2,\n",
    "            convert_system_message_to_human=True\n",
    "        )\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"You are an expert assistant. Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "1. Summary:  \n",
    "   Provide a succinct explanatory summary (1‚Äì4 sentences).\n",
    "\n",
    "2. Key Points:  \n",
    "   List the main supporting details in bullet form.\n",
    "\n",
    "Example format:\n",
    "\n",
    "1. Summary:  \n",
    "   The primary purpose of vector databases is to store and query dense vector embeddings for similarity search.\n",
    "\n",
    "2. Key Points:  \n",
    "   - Vector databases offer fully managed services, eliminating infrastructure overhead.  \n",
    "   - They support cosine and dot-product similarity metrics for fast nearest-neighbor retrieval.  \n",
    "   - They integrate seamlessly with popular embedding libraries like SentenceTransformer.  \n",
    "   - They provide automatic indexing to scale to millions of vectors.\n",
    "\n",
    "Now, answer the question below following this format:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create the RAG Chain\n",
    "\n",
    "Now we'll connect all components to create our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if vectorstore exists\n",
    "if 'vectorstore' in locals():\n",
    "    # Create RetrievalQA chain\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG chain with Gemini is ready.\")\n",
    "else:\n",
    "    print(\"Vectorstore not created. Please run previous cells successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test with Example Questions\n",
    "\n",
    "Let's test our RAG system with some example questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle questions\n",
    "def ask_question(question):\n",
    "    if 'qa_chain' not in locals() and 'qa_chain' not in globals():\n",
    "        print(\"RAG chain not created. Please run previous cells successfully first.\")\n",
    "        return None\n",
    "    \n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nANSWER:\")\n",
    "    print(result[\"result\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Interactive question loop\n",
    "def interactive_qa_session():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INTERACTIVE Q&A SESSION\")\n",
    "    print(\"Type 'exit', 'quit', or 'q' to end the session\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_question = input(\"\\nEnter your question: \")\n",
    "        \n",
    "        # Check if user wants to exit\n",
    "        if user_question.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"\\nEnding Q&A session. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process the question\n",
    "        ask_question(user_question)\n",
    "\n",
    "# Start the interactive session\n",
    "if 'qa_chain' in locals() or 'qa_chain' in globals():\n",
    "    interactive_qa_session()\n",
    "else:\n",
    "    print(\"RAG chain not created. Please run previous cells successfully first.\")\n",
    "    \n",
    "    # Optional: Ask if user wants to try an example question anyway\n",
    "    try_example = input(\"Would you like to try an example question anyway? (y/n): \")\n",
    "    if try_example.lower() in ['y', 'yes']:\n",
    "        example_question = \"What is the Anti-Corruption Layer (ACL) pattern\"\n",
    "        result = ask_question(example_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctse-qdrant-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
